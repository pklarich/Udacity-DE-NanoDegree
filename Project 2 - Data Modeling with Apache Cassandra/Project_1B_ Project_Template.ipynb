{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace/event_data is your root directory. \n",
      "\n",
      "30 files found. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the filepath for the provided folder_ext\n",
    "\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "print('{} is your root directory. \\n'.format(filepath))\n",
    "\n",
    "# Gather all of the file paths for .csv files specifically\n",
    "# in our defined path\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    # Looks like there is this folder hidden/created during\n",
    "    # the project so this is instituted to ignore it \n",
    "    if ('.ipynb_checkpoints' in root):\n",
    "        continue\n",
    "    file_path_list = glob.glob(os.path.join(root,'*.csv'))   \n",
    "\n",
    "path_len = len(file_path_list)\n",
    "print('{} files found. \\n'.format(path_len))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data has been processed into a single list data type. \n",
      "\n",
      "event_datafile_new.csv already exists and has been archived as 2021_11_13.csv. \n",
      "\n",
      "event_data has been processed and stored in event_datafile_new. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list containing all of the data from the list\n",
    "# of files that were found\n",
    "\n",
    "full_data_rows_list = [] \n",
    "for i, f in enumerate(file_path_list, 1): \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        for line in csvreader:\n",
    "            full_data_rows_list.append(line)\n",
    "print('File data has been processed into a single list data type. \\n')\n",
    "\n",
    "# If a data file already exists we will rename it with todays date and\n",
    "# place it into an archive folder. If this is run multiple times in a \n",
    "# day the archived file will be overwritten.\n",
    "\n",
    "if os.path.exists('event_datafile_new.csv'):\n",
    "    tday = datetime.now().strftime('%Y_%m_%d')\n",
    "    os.makedirs('Archive',exist_ok = True)\n",
    "    os.rename('event_datafile_new.csv','Archive/{}.csv'.format(tday))\n",
    "    print('event_datafile_new.csv already exists and has been archived as {}.csv. \\n'.format(tday))\n",
    "\n",
    "\n",
    "# Create a dialect to write a new csv file (event_datafile_new) with\n",
    "# making everything a string data type. The first row is a header row\n",
    "# and we only pull relevant columns from *full_data_rows_list*. If the\n",
    "# first row is empty it is know to not be relevant event data.\n",
    "\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True) \n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName',\\\n",
    "                    'length','level','location','sessionId','song','userId'])\n",
    "    for i, row in enumerate(full_data_rows_list, 1):\n",
    "        if (row[0] == ''): \n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7],\\\n",
    "                         row[8], row[12], row[13], row[16]))  \n",
    "\n",
    "print('event_data has been processed and stored in event_datafile_new. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f787172b390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there is no sparkify keyspace, build one.\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkify keyspace is created and set. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set our keyspace to sparkify, which was just created.\n",
    "session.set_keyspace('sparkify')\n",
    "\n",
    "print('sparkify keyspace is created and set. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Query Design\n",
    "\n",
    "1. Query 1 was designed as the \"session\" table. We need to filter by sessionId and itemInSession so these two columns were used as a composite key because the combinationis unique, and both are required filters for this query. Artist, song title and song length were all added as data columns as well.\n",
    "\n",
    "2. Query 2 was designed as the \"user\" table. We need to filter by userid and sessionid so these two columns were used as a composite key because the combination is unique and both are necessary as filters. ItemInSession is then used as a clustering column so the return is sorted by this column. Artist, song title, user first, and user last name are all added as data columns as well.\n",
    "\n",
    "3. Query 3 was designed as the \"song\" table. We need to filter by song, but that doesn't provide a unique id to each row. Users may have listened to the song multiple times. To make sure our Primary Key isn't used on multiple rows, sessionID and itemInSession are used as clustering columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# CREATE TABLES\n",
    "\n",
    "session_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "session_library(\n",
    "sessionId int,\n",
    "itemInSession int,\n",
    "artist text, \n",
    "length decimal,\n",
    "song text,\n",
    "PRIMARY KEY (sessionId, itemInSession)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "user_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS \n",
    "user_library( \n",
    "userId int,  \n",
    "sessionId int,\n",
    "itemInSession int,\n",
    "artist text, \n",
    "song text, \n",
    "firstName text,  \n",
    "lastName text,\n",
    "PRIMARY KEY ((userId,sessionId), itemInSession)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "song_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "song_library( \n",
    "song text,\n",
    "sessionId int,\n",
    "itemInSession int,\n",
    "firstName text,\n",
    "lastName text,\n",
    "PRIMARY KEY (song,sessionId,itemInSession)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# DROP TABLES\n",
    "\n",
    "session_drop = (\"\"\"DROP TABLE IF EXISTS session_library\"\"\")\n",
    "\n",
    "user_drop = (\"\"\"DROP TABLE IF EXISTS user_library\"\"\")\n",
    "\n",
    "song_drop = (\"\"\"DROP TABLE IF EXISTS song_library\"\"\")\n",
    "\n",
    "\n",
    "# INSERT RECORDS\n",
    "\n",
    "session_insert =(\"\"\"\n",
    "INSERT INTO \n",
    "session_library(\n",
    "sessionId, \n",
    "itemInSession, \n",
    "artist, \n",
    "length, \n",
    "song\n",
    ") \n",
    "VALUES(%s,%s,%s,%s,%s)\n",
    "\"\"\")\n",
    "user_insert =   (\"\"\"\n",
    "INSERT INTO user_library(\n",
    "userId\n",
    "sessionId, \n",
    "itemInSession, \n",
    "artist, \n",
    "song, \n",
    "firstName, \n",
    "lastName, \n",
    ") \n",
    "VALUES(%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\")\n",
    "\n",
    "song_insert =   (\"\"\"\n",
    "INSERT INTO song_library(\n",
    "song, \n",
    "sessionId, \n",
    "itemInSession\n",
    "firstName, \n",
    "lastName, \n",
    ") \n",
    "VALUES(%s,%s,%s,%s,%s)\n",
    "\"\"\")\n",
    "\n",
    "# TEST QUERIES\n",
    "\n",
    "session_query = (\"\"\" \n",
    "SELECT artist, song, length \n",
    "FROM session_library \n",
    "WHERE sessionId = 338 AND itemInSession = 4\n",
    "\"\"\")\n",
    "\n",
    "user_query = (\"\"\"\n",
    "SELECT artist, song, firstName, lastName \n",
    "FROM user_library \n",
    "WHERE userId = 10 AND sessionId = 182\n",
    "\"\"\")\n",
    "\n",
    "song_query = (\"\"\"\n",
    "SELECT firstName, lastName \n",
    "FROM song_library \n",
    "WHERE song = 'All Hands Against His Own'\n",
    "\"\"\")\n",
    "\n",
    "query1 = (\"\"\"\n",
    "Give me the artist, song title and song's length in the music app history\n",
    "that was heard during sessionId = 338, and itemInSession = 4\n",
    "\"\"\")\n",
    "\n",
    "query2 = (\"\"\"\n",
    "Give me only the following: name of artist, song (sorted by itemInSession)\n",
    "and user (first and last name) for userid = 10, sessionid = 182\n",
    "\"\"\")\n",
    "\n",
    "query3 = (\"\"\"\n",
    "Give me every user name (first and last) in my music app history who listened\n",
    "to the song 'All Hands Against His Own'\n",
    "\"\"\")\n",
    "\n",
    "drop_table_queries = {'session_drop': session_drop, 'user_drop': user_drop, 'song_drop': song_drop}\n",
    "create_table_queries = {'session_create': session_create, 'user_create':user_create, 'song_create':song_create}\n",
    "insert_queries = {'session_insert':session_insert,'user_insert': user_insert,'song_insert': song_insert}\n",
    "check_queries = {query1: session_query, query2: user_query, query3: song_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session table in the sparkify keyspace has been added.\n",
      "user table in the sparkify keyspace has been added.\n",
      "song table in the sparkify keyspace has been added.\n"
     ]
    }
   ],
   "source": [
    "# Create any pre-defined tables from the sql_queries 'library'\n",
    "for query in create_table_queries:\n",
    "    session.execute(create_table_queries[query])\n",
    "    print('{} table in the sparkify keyspace has been added.'.format(query[:-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant data from event_datafile_new.csv has been uploaded to the sparkify keyspace. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open our event data file and read it line by line, uploading relevant data\n",
    "with open('event_datafile_new.csv', encoding = 'utf8') as f:\n",
    "    rowtot = sum(1 for lines in f)\n",
    "with open('event_datafile_new.csv', encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # Skip header\n",
    "    printcount = 0\n",
    "    for i, line in enumerate(csvreader,1):\n",
    "        session.execute(insert_queries['session_insert'], (line[0], int(line[3]),\\\n",
    "                        float(line[5]), int(line[8]), line[9]))\n",
    "        session.execute(insert_queries['user_insert'], (line[0], line[9], line[1],\\\n",
    "                        int(line[3]), line[4], int(line[8]), int(line[10])))\n",
    "        session.execute(insert_queries['song_insert'], (line[1], line[4], line[9], \\\n",
    "                        int(line[8]), int(line[3])))\n",
    "        printcount += 1\n",
    "        if printcount == 1000 or i == rowtot:\n",
    "            print('{}/{} rows uploaded to keyspace'.format(i,rowtot), end = \"\\r\", flush = True)\n",
    "            printcount = 0\n",
    "\n",
    "print('Relevant data from event_datafile_new.csv has been uploaded to the sparkify keyspace. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query #1:\n",
      "\n",
      "Give me the artist, song title and song's length in the music app history\n",
      "that was heard during sessionId = 338, and itemInSession = 4\n",
      ":\n",
      "\n",
      "\n",
      "      artist                             song    length\n",
      "0  Faithless  Music Matters (Mark Knight Dub)  495.3073\n",
      "\n",
      "Query #2:\n",
      "\n",
      "Give me only the following: name of artist, song (sorted by itemInSession)\n",
      "and user (first and last name) for userid = 10, sessionid = 182\n",
      ":\n",
      "\n",
      "\n",
      "              artist                                               song  \\\n",
      "0   Down To The Bone                                 Keep On Keepin' On   \n",
      "1       Three Drives                                        Greece 2000   \n",
      "2  Sebastien Tellier                                          Kilometer   \n",
      "3      Lonnie Gordon  Catch You Baby (Steve Pitron & Max Sanna Radio...   \n",
      "\n",
      "  firstname lastname  \n",
      "0    Sylvie     Cruz  \n",
      "1    Sylvie     Cruz  \n",
      "2    Sylvie     Cruz  \n",
      "3    Sylvie     Cruz  \n",
      "\n",
      "Query #3:\n",
      "\n",
      "Give me every user name (first and last) in my music app history who listened\n",
      "to the song 'All Hands Against His Own'\n",
      ":\n",
      "\n",
      "\n",
      "    firstname lastname\n",
      "0        Sara  Johnson\n",
      "1  Jacqueline    Lynch\n",
      "2       Tegan   Levine\n",
      "\n",
      "\n",
      "All queries have ran\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,query in enumerate(check_queries,1):\n",
    "    rows = session.execute(check_queries[query])\n",
    "    print('Query #{}:\\n{}:\\n'.format(i,query))\n",
    "    df = pd.DataFrame(list(rows))\n",
    "    print('\\n{}\\n'.format(df))\n",
    "print('\\nAll queries have ran\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session table in the sparkify keyspace has been dropped.\n",
      "user table in the sparkify keyspace has been dropped.\n",
      "song table in the sparkify keyspace has been dropped.\n"
     ]
    }
   ],
   "source": [
    "# Drop any pre-defined tables from the sql_queries 'library'\n",
    "for query in drop_table_queries:\n",
    "    session.execute(drop_table_queries[query])\n",
    "    print('{} table in the sparkify keyspace has been dropped.'.format(query[:-5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session is shutdown \n",
      "\n",
      "cluster is shutdown \n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.shutdown()\n",
    "print('session is shutdown \\n')\n",
    "cluster.shutdown()\n",
    "print('cluster is shutdown \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
